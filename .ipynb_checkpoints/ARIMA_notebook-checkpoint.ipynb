{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ARIMA: Autoregressive Integrated Moving Average\n",
    "\n",
    "## By Team Fourier\n",
    "CS189 Project T Final\n",
    "\n",
    "This notebook will provide an introduction into ARIMA, short for autoregressive integrated moving average. ARIMA models aim to model time series data such as the weather, stock prices, or product sales over a period of time. The focus will be on weather data which has a seasonal aspect to it. The notebook will move into SARIMAX, short for seasonal autoregressive integrated moving average, a model that allows for seasonal periodic behavior. The notebook will rely heavily on the statsmodels Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Provide some insight into the power of time series analysis and moving average models as opposed to traditional function fitting techniques such as linear regression.\n",
    "\n",
    "2. Provide a brief introduction to techniques in order to determine important characteristics of your data (stationarity, seasonality) that will influence decisions when choosing a model.\n",
    "\n",
    "3. Give an introduction into statsmodels ARIMA and SARIMAX capabilities. In real applications, these functions are what you would use to implement an ARIMA model yourself.\n",
    "\n",
    "4. Provide some examples of visualization, an important skill for data scientists.\n",
    "\n",
    "5. A brief example of using grid search in order to choose optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/TSLA.csv')\n",
    "time = np.array(data['Date'])\n",
    "price = np.array(data['Close'])\n",
    "\n",
    "plt.title('Tesla Stock Price')\n",
    "plt.plot(time, price)\n",
    "plt.xticks(time[::50])\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the graph above depicts the stock price of TSLA over time. The x-axis being time and the y-axis being f(t), the price at that point in time.\n",
    "\n",
    "Clearly a basic linear model wouldn't be able to model such a function of time due to its non-linearity. However, we have previously learned ways to fit non-linear functions with linear regression using feature lifting. Maybe lifting some polynomial features might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Linear Regression\n",
    "\n",
    "Linear features, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = np.arange(0, time.shape[0], 1).astype(np.int).reshape((time.shape[0], 1))\n",
    "\n",
    "#TODO: run a least squares linear regression in the format Ax=b, A = time_steps, B = price\n",
    "#place the result in w\n",
    "#Hint: try np.linalg.lstsq\n",
    "w = 0\n",
    "\n",
    "plt.plot(time_steps, w * time_steps, label='linear fit')\n",
    "plt.plot(time_steps, price, label = 'original data')\n",
    "plt.legend()\n",
    "plt.title('Tesla Stock Price with linear fit')\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_lift(X, d=2):\n",
    "    \"\"\"\n",
    "    This function computes second order variables\n",
    "    for polynomial regression.\n",
    "    Input:\n",
    "    X: Independent variables.\n",
    "    Output:\n",
    "    A data matrix composed of both first and second order terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(X)\n",
    "    X_data = []\n",
    "    \n",
    "    for deg in range(1, d+1):\n",
    "        X_data.append(np.power(X, deg))\n",
    "        \n",
    "    return np.hstack(X_data)\n",
    "\n",
    "second_order_time_steps = feature_lift(time_steps, d=2)\n",
    "w_2, err_2, _, _ = np.linalg.lstsq(second_order_time_steps[:210], price[:210])\n",
    "\n",
    "third_order_time_steps = feature_lift(time_steps, d=3)\n",
    "w_3, err_3, _, _ = np.linalg.lstsq(third_order_time_steps[:220], price[:220])\n",
    "\n",
    "fifty_order_time_steps = feature_lift(time_steps, d=50)\n",
    "w_50, err_50, _, _ = np.linalg.lstsq(fifty_order_time_steps[:220], price[:220])\n",
    "\n",
    "plt.plot(time_steps, second_order_time_steps @ w_2.T, label='deg 2 fit')\n",
    "plt.plot(time_steps, third_order_time_steps @ w_3.T, label='deg 3 fit')\n",
    "plt.plot(time_steps, fifty_order_time_steps @ w_50.T, label='deg 50 fit')\n",
    "plt.plot(time_steps, price, label = 'original data')\n",
    "\n",
    "plt.xlim(right=250)\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.title('Tesla stock price with linear regression on feature lift')\n",
    "plt.show()\n",
    "\n",
    "print('Training error with degree 2 features: %s' % str(err_2))\n",
    "print('Training error with degree 3 features: %s' % str(err_3))\n",
    "print('Training error with degree 50 features: %s' % str(err_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, any attempt to fit some sort of function using linear regression and feature lifting will be met with failure. Why? This is due to the nature of time series data. The problem that linear regressions try to solve is fitting some function to the data in its entirety. However, time series data, such as stock prices don't operate under the same assumptions. Does the stock price 1 year ago directly affect the stock price tomorrow? The answer is, not really. This understanding that only the previous n' << n observations are truly important to our model spawns this set of models that utilize the moving average, the first MA model, then the ARMA, and ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation of the ARIMA Model\n",
    "\n",
    "The mathematical model behind ARIMA is:\n",
    "\n",
    "\\begin{align*}\n",
    "ARIMA(p, d, q) = (1 - \\phi_{1}L - \\phi_{2}L^{2} - \\phi_{3}L^{3}... - \\phi_{p}L^{p}) (1 - L)^{d} y_{t} = c + (1 + \\theta_{1} L + \\theta_{2} L^{2}... + \\theta_{q} L^{q}) \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "The L is the lag operator, $L^d y_{t} = y_{t - d}$.\n",
    "\n",
    "The first group of terms, containing $\\phi$'s, are the auto-regressive terms. Auto-regressive refers to the contribution of previous observations on the current observation. The parameter $p$ is the number of previous observations looked at. The $\\phi$'s are their individual weighting. Intuitively, an observation 1 time step ago usually holds more weight than an observation 10 time steps ago.\n",
    "\n",
    "The second term, $(1 - L)^{d}$, is the integrated term. This is the term that implements the idea of differencing, which is sometimes necessary to find a deeper relationship in the data. The time series function may not be able to model the observation themselves, but may be able to model the difference between successive observations.\n",
    "\n",
    "The third and final term, containing $\\theta$'s, represent the moving average portion of the model. This is the contribution of the previous $q$ terms, specifically their residuals to the residual at time t. The residuals, $\\epsilon_{t}$, are standard normal distribution deviations of each observation from the predicted observation predicted by the model. These residuals are what we try to minimize when fitting parameters to the training data.\n",
    "\n",
    "The constant c quantifies the drift of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Down the ARIMA Model\n",
    "\n",
    "## Auto-Regressive (AR)\n",
    "\n",
    "One of the fundamental building blocks of the ARIMA model is the AR portion of the model. The equation for a simple AR model is:\n",
    "\n",
    "\\begin{align*}\n",
    "AR(p) = y_{t} = c + (\\phi_{1}L + \\phi_{2}L^{2} + \\phi_{3}L^{3}... + \\phi_{p}L^{p})y_{t} + \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "This equation can already begin to model functions under the assumption that current observations are simply derived from a linear combination of the p previous observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this notebook, we'll be working with weather data instead of stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "weather_data = pd.read_csv('data/weather_data.csv', parse_dates=['datetime_utc'], index_col='datetime_utc')\n",
    "weather_data = weather_data.rename(index=str, columns={' _tempm': 'temperature'})\n",
    "\n",
    "#interpolate null values\n",
    "weather_data.ffill(inplace=True)\n",
    "weather_data.index = pd.to_datetime(weather_data.index)\n",
    "\n",
    "#remove outliers\n",
    "weather_data = weather_data[weather_data.temperature < 50]\n",
    "weather_temp = weather_data['temperature']\n",
    "\n",
    "weather_temp.plot()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('temp in deg C')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Historical weather data')\n",
    "plt.show()\n",
    "\n",
    "print(weather_temp.head())\n",
    "print('Null values: %s' % str(weather_temp.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do with our data is clean it, interpolating null values and making sure the samples are evenly spaced. Next, there are some things we can check about the data itself to ensure it is suitable for an ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = weather_temp['2000':'2014'].resample('M').mean().fillna(method='pad')\n",
    "testing_data = weather_temp['2015':'2017'].resample('M').mean().fillna(method='pad')\n",
    "training_data.plot()\n",
    "testing_data.plot()\n",
    "plt.title('Training and Testing Data Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will plot the autocorrelation of both the original data and the differences between consecutive data points. There is also a provided function ``plot_rolling_mean_std``. Try to find a period length that creates a constant rolling mean. What does this say about the periodicity of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rolling mean and rolling standard deviation\n",
    "def plot_rolling_mean_std(ts, period):\n",
    "    rolling_mean = ts.rolling(period).mean()\n",
    "    rolling_std = ts.rolling(period).std()\n",
    "\n",
    "    plt.plot(ts, label='Actual Mean')\n",
    "    plt.plot(rolling_mean, label='Rolling Mean')\n",
    "    plt.plot(rolling_std, label = 'Rolling Std')\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Mean Temperature\")\n",
    "    plt.title('Rolling Mean & Rolling Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#TODO: change this period parameter until the resulting rolling mean is 0 (Hint: What do you think the period of this data is?)\n",
    "plot_rolling_mean_std(training_data, period = 2)\n",
    "    \n",
    "plt.plot(training_data)\n",
    "plot_acf(training_data)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_data.diff())\n",
    "plot_acf(training_data.diff().values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the rolling mean and rolling standard deviation plot that the data has a constant mean and standard deviation on a 12 month period. This periodicity implies that our data is seasonal, meaning a standard ARIMA model will have trouble fitting it, but instead we will need to use the seasonal aspect of SARIMA to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try to see what kind of model we retrieve from an AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(res, model='arima'):\n",
    "    print(res.summary())\n",
    "    \n",
    "    plt.plot(res.resid)\n",
    "    plt.title('Training Residuals')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "\n",
    "    print('Mean squared training error: %s' % str(np.square(res.resid).mean()))\n",
    "\n",
    "    plt.plot(res.predict(), label='Prediction')\n",
    "    plt.plot(training_data.values, label='Real values')\n",
    "    plt.legend()\n",
    "    plt.title('Model fit on training data')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "    \n",
    "    if model == 'arima':\n",
    "        fc, se, conf = res.forecast(testing_data.shape[0], alpha=0.05)\n",
    "    elif model == 'sarimax':\n",
    "        fc = res.forecast(testing_data.shape[0], alpha=0.05)\n",
    "        \n",
    "    fc_series = pd.Series(fc, index=testing_data.index)\n",
    "    plt.plot(fc_series, label='Prediction')\n",
    "    plt.plot(testing_data, label='Real values')\n",
    "    plt.legend()\n",
    "    plt.title('Model prediction on test data')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO: Calculate the mean squared testing error\n",
    "    squared_testing_error = np.square(fc - testing_data).mean()\n",
    "    print('Mean squared testing error: %s' % str(squared_testing_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use ARIMA(data, order).fit() to generate an AR model fit\n",
    "#data = training_data.values\n",
    "#order = (p, 0, 0)\n",
    "#Hint: try p > 1\n",
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html\n",
    "res = None\n",
    "process_result(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Why does the AR model not fit this seasonal data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average (MA)\n",
    "\n",
    "Another fundamental building block of the ARIMA model is the MA portion of the model. The equation for a simple MA model is:\n",
    "\n",
    "\\begin{align*}\n",
    "MA(q) = y_{t} = c + \\epsilon_{t} + \\theta_{1}\\epsilon_{t-1} + \\theta_{2}\\epsilon_{t-2} + ... + \\theta_{q}\\epsilon_{t-q}\n",
    "\\end{align*}\n",
    "\n",
    "This equation models the assumption that the residual, the deviation from the predicted point, of the current observation is a linear combination of the previous q residuals.\n",
    "\n",
    "Add the moving average section of the ARIMA model, and model the weather data using an ARMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use ARIMA(data, order).fit() to generate an ARMA model fit\n",
    "#data = training_data.values\n",
    "#order = (p, 0, q)\n",
    "#Hint: try p, q > 1\n",
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html\n",
    "res = None\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Does the ARMA model fit the seasonal data better than the AR model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality in ARIMA: SARIMA\n",
    "\n",
    "As we noted in the weather data, we see a seasonal pattern, or period with fixed period length. The traditional ARIMA model is not fully suited to model this behavior, but this is where SARIMA comes into play. SARIMA, season auto-regressive integrated moving average, has a slightly different formulation from ARIMA:\n",
    "\n",
    "\\begin{align*}\n",
    "SARIMA(p, d, q) (P, D, Q)_{m} = (1 - \\phi_{1}L - \\phi_{2}L^{2} - \\phi_{3}L^{3}... - \\phi_{p}L^{p}) (1 - \\Phi_{1}L^{m} - \\Phi_{2}L^{2m} - \\Phi_{3}L^{3m}... - \\Phi_{P}L^{Pm}) (1 - L)^{d} (1 - L)^{Dm} y_{t} = c + (1 + \\theta_{1} L + \\theta_{2} L^{2}... + \\theta_{q} L^{q}) (1 + \\Theta_{1} L^{m} + \\Theta_{2} L^{2m}... + \\Theta_{Q} L^{Qm}) \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "The seasonal terms are terms that operate much like the original ARIMA terms, but the lag operators are all raised to the power of $m$, in the case of the annual cycle of weather data, $m=12$, meaning the model takes into account observations from 12, 24, 36.... months ago.\n",
    "\n",
    "Implement a SARIMA model on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use SARIMAX(data, order, seasonal_order).fit() to generate a SARIMAX model fit\n",
    "#data = training_data.values\n",
    "#order = (p, 0, q)\n",
    "#seasonal_order = (P, 0, Q, S)\n",
    "#Hint: try p, q > 1 and P, Q > 0\n",
    "#https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
    "res = None\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Does the SARIMAX model fit the seasonal data better than the ARIMA model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "You may have noticed that sometimes the performance is better than other times when using different values for p, d, q, P, D, and Q. These values are called hyperparameters, hand selected values by the engineer creating the model that greatly influence the effectiveness of the model. There are several ways to set these hyperparameters, as discussed earlier in this course. You can employ techniques such as k-fold cross validation and grid searches, as well as use domain knowledge to make educated guesses on what we want the model to learn and what the underlying model in reality is. For example, someone with strong knowledge of the weather can make a better educated guess about how many days one should look back in the past to predict today's weather."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make a guess based on what you think influences the weather. What are good values for p, d, q, P, D, and Q? \n",
    "\n",
    "Hint: Don't use d or D, differencing leads to a very bad result in this dataset.\n",
    "\n",
    "Run a SARIMAX model with your guess here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO run a SARIMAX model with your guess\n",
    "res = None\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try using a grid search to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_mse(res):\n",
    "    #TODO calculate test mse\n",
    "    #refer to function process_result in this notebook on how to use res.forecast()\n",
    "    fc = None\n",
    "    squared_testing_error = np.square(fc - testing_data).mean()\n",
    "    return squared_testing_error\n",
    "\n",
    "p_range = list(range(4))\n",
    "q_range = list(range(4))\n",
    "P_range = list(range(4))\n",
    "Q_range = list(range(4))\n",
    "\n",
    "best_params = (-1, -1, -1, -1)\n",
    "best_test_mse = 10000000\n",
    "\n",
    "#todo use a grid search to find the best parameters\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        for P in P_range:\n",
    "            for Q in Q_range:\n",
    "                try:\n",
    "                    res = SARIMAX(training_data.values, order=(p, 0, q), seasonal_order=(P, 0, Q, 12), maxiter = 5).fit()\n",
    "                    test_mse = get_test_mse(res)\n",
    "                    if test_mse < best_test_mse:\n",
    "                        best_params = (p, q, P, Q)\n",
    "                        best_test_mse = test_mse\n",
    "                    print('Tested params:', p, q, P, Q)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "print('Best parameters: ', best_params)\n",
    "res = SARIMAX(training_data.values, order=(best_params[0], 0, best_params[1]), seasonal_order=(best_params[2], 0, best_params[3], 12)).fit()\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That probably took a while, was it worth it?\n",
    "\n",
    "## Observations\n",
    "\n",
    "How did the grid search perform compared to your guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
