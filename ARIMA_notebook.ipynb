{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ARIMA: Autoregressive Integrated Moving Average\n",
    "\n",
    "## By Team Fourier\n",
    "\n",
    "This notebook will provide an introduction into ARIMA, short for autoregressive integrated moving average. ARIMA models aim to model time series data such as the weather, stock prices, or product sales over a period of time. The focus will be on weather data which has a seasonal aspect to it. The notebook will move into SARIMAX, short for seasonal autoregressive integrated moving average, a model that allows for seasonal periodic behavior. The notebook will rely heavily on the statsmodels Python package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Provide some insight into the power of time series analysis and moving average models as opposed to traditional function fitting techniques such as linear regression.\n",
    "\n",
    "2. Provide a brief introduction to techniques in order to determine important characteristics of your data (stationarity, seasonality) that will influence decisions when choosing a model.\n",
    "\n",
    "3. Give an introduction into statsmodels ARIMA and SARIMAX capabilities. In real applications, these functions are what you would use to implement an ARIMA model yourself.\n",
    "\n",
    "4. Provide some examples of visualization, an important skill for data scientists.\n",
    "\n",
    "5. A brief example of using grid search in order to choose optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Smoothing\n",
    "\n",
    "We can think of timeseries data as the following form:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{t} = m_{t} + Z_{t}\n",
    "\\end{align*}\n",
    "\n",
    "This formulation of timeseries data means there is an underlying model function $m(y)$ and residuals at each observation point: $Z_{t}$. The purpose of timeseries analysis is to predict these $Z_{t}$'s using a combination of the previous $p$ or $q$ $Z_{t}$'s or the observations themselves. In this case, having an $m_{t}$ is inconvenient for our model as we only want to predict $Z_{t}$'s. Therefore, we can perform a technique called smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 main ways to perform smoothing:\n",
    "\n",
    "1. Perform a linear or quadratic fit on the data, then simply reduce the model to $Y_{t} = X_{t} - f(t)$ which becomes $Y_{t} = Z_{t}$ if $f(t) = m_{t}$. \n",
    "\n",
    "2. Perform smoothing using a moving average fit.\n",
    "\n",
    "Derive $m_{t}$ using the formula: \n",
    "\n",
    "\\begin{align*}\n",
    "m_{t} = \\frac{1}{2q+1} + \\sum_{j=-q}^{q} X_{t+j}\n",
    "\\end{align*}\n",
    "\n",
    "Then obtain the new observations: $Y_{t} = X_{t} - m_{t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Perform smoothing using regression\n",
    "\n",
    "Implement smoothing using regression to find a linear or quadratic fit on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_timeseries(deg = 'linear', samples = 100):\n",
    "    if deg == 'linear':\n",
    "        w = np.random.uniform(2, 5)\n",
    "        X = np.arange(samples).astype(np.float32)\n",
    "        X *= w\n",
    "    elif deg == 'quadratic':\n",
    "        w = np.random.uniform(0.03, 0.08, size=(2,1))\n",
    "        X = np.vstack((np.arange(samples), np.square(np.arange(samples)))).astype(np.float32).T\n",
    "        X = X @ w\n",
    "    elif deg == 'sinusoidal':\n",
    "        period = 20\n",
    "        w = np.random.uniform(50, 100)\n",
    "        X = np.sin(np.arange(samples).astype(np.float32) * 2 * np.pi / period)\n",
    "        X *= w\n",
    "    else:\n",
    "        print('only support deg = linear, quadratic, sinusoidal')\n",
    "        \n",
    "    X = np.random.normal(X, samples / 10)\n",
    "    X = X.reshape((X.shape[0], 1))\n",
    "    return X\n",
    "\n",
    "\n",
    "def plot_timeseries(ts, label):\n",
    "    plt.plot(ts, label=label)\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('observations')\n",
    "\n",
    "\n",
    "def smoothing_via_regression(X, fit=\"linear\"):\n",
    "    plot_timeseries(X, 'Timeseries Data Before Smoothing')\n",
    "    \n",
    "    #TODO: implement a linear or quadratic regression fit\n",
    "    Y = ... #output Y = X - m\n",
    "    m = ... #fitted model, should be the same shape as X\n",
    "    \n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    if fit == 'linear':\n",
    "        time = ...\n",
    "        w = ...\n",
    "        m = ...\n",
    "        Y = ...\n",
    "        \n",
    "        \n",
    "    elif fit == 'quadratic':\n",
    "        time = ...\n",
    "        w = ...\n",
    "        m = ...\n",
    "        Y = ...\n",
    "    #END YOUR CODE\n",
    "        \n",
    "    else:\n",
    "        print('fit must be one of \"linear\" or \"quadratic\"')\n",
    "        \n",
    "    plot_timeseries(Y, 'Timeseries Data After Model Fit')\n",
    "    plot_timeseries(m, 'Fitted Model')\n",
    "    plt.legend()\n",
    "    plt.title(\"Smoothing using %s regression\" % fit)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Perform smoothing using moving average\n",
    "\n",
    "Implement smoothing using a moving average to approximate $m_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_via_moving_average(X, q=1):\n",
    "    plot_timeseries(X, 'Timeseries Data Before Smoothing')\n",
    "    \n",
    "    assert(X.shape[0] > 2 * q + 1)\n",
    "    \n",
    "    #TODO: implement a moving average fit using the parameter q\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    #try using np.convolve(mode=\"same\"), don't worry about weird values on the boundaries\n",
    "    \n",
    "    m = None #fitted model, same shape as X\n",
    "    Y = None #output Y = X - m\n",
    "    \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    plot_timeseries(Y, 'Timeseries Data After Model Fit')\n",
    "    plot_timeseries(m, 'Fitted Model')\n",
    "    plt.legend()\n",
    "    plt.title(\"Smoothing using moving average with parameter q=%d\" % q)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linear = generate_timeseries(deg='linear')\n",
    "smoothing_via_regression(X_linear, fit='linear')\n",
    "smoothing_via_moving_average(X_linear, q=3)\n",
    "\n",
    "\n",
    "X_quad = generate_timeseries(deg='quadratic')\n",
    "smoothing_via_regression(X_quad, fit='quadratic')\n",
    "smoothing_via_moving_average(X_quad, q=3)\n",
    "\n",
    "X_sin = generate_timeseries(deg='sinusoidal')\n",
    "smoothing_via_regression(X_sin, fit='linear')\n",
    "smoothing_via_regression(X_sin, fit='quadratic')\n",
    "smoothing_via_moving_average(X_sin, q=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Test Smoothing on a Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/TSLA.csv')\n",
    "time = np.array(data['Date'])\n",
    "price = np.array(data['Close'])\n",
    "\n",
    "plt.title('Tesla Stock Price')\n",
    "plt.plot(time, price)\n",
    "plt.xticks(time[::50])\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "\n",
    "price = price.reshape((price.shape[0], 1))\n",
    "\n",
    "smoothing_via_regression(price, fit='linear')\n",
    "smoothing_via_moving_average(price, q=5)\n",
    "smoothing_via_regression(price, fit='quadratic')\n",
    "smoothing_via_moving_average(price, q=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. Which smoothing performed better using the synthetic data?\n",
    "\n",
    "2. Which smoothing performed better using the real Tesla data? Why?\n",
    "\n",
    "3. Try using different values of q, what happens when the value of q becomes very large. What happens when it is too small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Stationarity and Differencing\n",
    "\n",
    "Smoothing is one way to enforce stationarity on the timeseries data. What is stationarity?  \n",
    "Stationarity is the idea that the data is the same no matter where you look. Formally, that is the following equation is the same for the joint distributions given any $t$, $q$, $n$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(X_{t}, ..., X_{t+n}) = P(X_{q}, ..., X_{q+n})\n",
    "\\end{align*}\n",
    "\n",
    "For data, you can assess this qualitatively: No obvious trend, and no seasonality. If the data, however, has some sort of trend or seasonality, we can get rid of them via differencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Trend Differencing\n",
    "\n",
    "If our data has a trend, meaning it can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{t} = m_{t} + Z_{t}\n",
    "\\end{align*}\n",
    "\n",
    "Differencing is a technique in which we can create a new timeseries:\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{t} = X_{t} - X_{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "If our trend is linear, for instance, we can achieve the following result using differencing:\n",
    "\n",
    "\\begin{align*}\n",
    "m_{t} = at + b\\\\\n",
    "X_{t} = m_{t} + Z_{t}\\\\\n",
    "X_{t-1} = m_{t-1} + Z_{t-1}\\\\\n",
    "Y = X_{t} - X_{t-1} = a + Z_{t} - Z_{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "We see that in this case, the trend has been completely removed, $at$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform differencing to eliminate the trend from the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_removal_via_differencing(X, diff=1):\n",
    "    plot_timeseries(X, 'Timeseries Data Before Differencing')\n",
    "    \n",
    "    #TODO: implement regular differencing\n",
    "    #Perform differencing diff times\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    Y = ...\n",
    "    for _ in range(diff):\n",
    "        \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    plot_timeseries(Y, 'Timeseries Data After Differencing')\n",
    "    plt.legend()\n",
    "    plt.title(\"Timeseries data after differencing %d times\" % diff)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "X_linear_trend = generate_timeseries(deg='linear')\n",
    "X_quad_trend = generate_timeseries(deg='quadratic')\n",
    "X_sin_trend = generate_timeseries(deg='sinusoidal')\n",
    "\n",
    "trend_removal_via_differencing(X_linear_trend)\n",
    "trend_removal_via_differencing(X_quad_trend, diff = 2)\n",
    "trend_removal_via_differencing(X_sin_trend, diff=1)\n",
    "\n",
    "#using differencing on Tesla data\n",
    "trend_removal_via_differencing(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "1. Which data did differencing perform well on?\n",
    "2. Which data did differencing perform poorly on?\n",
    "3. For linear data, what level of differencing should be used?\n",
    "4. For quadratic data, what level of differencing should be used? (Try the math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Seasonal Differencing\n",
    "\n",
    "You probably saw that differencing performed poorly on some of the data because it had a seasonal trend.\n",
    "\n",
    "If our data has a seasonal trend, meaning it can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{t} = m_{t} + Z_{t}\\\\\n",
    "m_{t} = m_{t-p}\n",
    "\\end{align*}\n",
    "\n",
    "where $p$ is the period of the seasonal trend.\n",
    "\n",
    "Seasonal differencing is a technique in which we can create a new timeseries:\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{t} = X_{t} - X_{t-p}\n",
    "\\end{align*}\n",
    "\n",
    "If our trend is linear, for instance, we can achieve the following result using differencing:\n",
    "\n",
    "\\begin{align*}\n",
    "m_{t} = m_{t-p}\\\\\n",
    "X_{t} = m_{t} + Z_{t}\\\\\n",
    "X_{t-p} = m_{t-p} + Z_{t-p}\\\\\n",
    "Y = X_{t} - X_{t-p} = Z_{t} - Z_{t-p}\n",
    "\\end{align*}\n",
    "\n",
    "We see that in this case, the trend has been completely removed by using the property of our trend's seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_removal_via_seasonal_differencing(X, diff=1, period = 1):\n",
    "    plot_timeseries(X, 'Timeseries Data Before Differencing')\n",
    "    \n",
    "    #TODO: implement seasonal differencing\n",
    "    #Perform differencing diff times\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    Y = ... #output\n",
    "    for _ in range(diff):\n",
    "        \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    plot_timeseries(Y, 'Timeseries Data After Seasonal Differencing')\n",
    "    plt.legend()\n",
    "    plt.title(\"Timeseries data after seasonal differencing %d times\" % diff)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "X_linear_trend = generate_timeseries(deg='linear')\n",
    "X_quad_trend = generate_timeseries(deg='quadratic')\n",
    "X_sin_trend = generate_timeseries(deg='sinusoidal')\n",
    "\n",
    "trend_removal_via_seasonal_differencing(X_linear_trend)\n",
    "trend_removal_via_seasonal_differencing(X_quad_trend)\n",
    "trend_removal_via_seasonal_differencing(X_sin_trend, period=20)\n",
    "\n",
    "#using differencing on Tesla data\n",
    "trend_removal_via_seasonal_differencing(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "1. Did the timeseries that trend differencing failed on work with seasonal differencing? With what parameter period?\n",
    "2. How does seasonal differencing work on linear/quadratic data? Derive what seasonal differencing does to data that is not actually seasonal. You should see that it effectively removes the trend while introducing more bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "Performing both seasonal and trend differencing on data can remove both seasonal and non-seasonal trends.\n",
    "\n",
    "If our data has a seasonal and non-seasonal trend, meaning it can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "X_{t} = m_{t} + s_{t} + Z_{t}\\\\\n",
    "s_{t} = s_{t-p}\n",
    "\\end{align*}\n",
    "\n",
    "where $p$ is the period of the seasonal trend. $m_{t}$ is the non-seasonal trend.\n",
    "\n",
    "Combining both seasonal and non-seasonal differencing is a technique in which we can create a new timeseries:  \n",
    "First, seasonal differencing:\n",
    "\\begin{align*}\n",
    "Y_{t} = m_{t} - m_{t-p} + Z_{t} - Z_{t-p}\n",
    "\\end{align*}\n",
    "\n",
    "Then, regular differencing:\n",
    "\\begin{align*}\n",
    "Y_{t} = m_{t} - m_{t-p} + Z_{t} - Z_{t-p}\\\\\n",
    "Y_{t-1} = m_{t-1} - m_{t-p-1} + Z_{t-1} - Z_{t-p-1}\\\\\n",
    "Y_{t} - Y_{t-1} = c + Z_{t} - Z_{t-p} + Z_{t-1} - Z_{t-p-1}\n",
    "\\end{align*}\n",
    "\n",
    "We see that by performing first seasonal differencing, and then regular differencing, we can remove both a seasonal and trend component from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_removal_via_seasonal_nonseasonal_differencing(X, diff=1, period = 1, seasonal_diff=1):\n",
    "    plot_timeseries(X, 'Timeseries Data Before Differencing')\n",
    "    \n",
    "    #TODO: Implement seasonal differencing first seasonal_diff times, then perform regular differencing diff times\n",
    "    #YOUR CODE HERE\n",
    "    Y = ... #output\n",
    "    for _ in range(seasonal_diff):\n",
    "        \n",
    "    for _ in range(regular_diff):\n",
    "    \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    plot_timeseries(Y, 'Timeseries Data After Both Differencing')\n",
    "    plt.legend()\n",
    "    plt.title(\"Timeseries data after both differencing: %d times regular, %d times seasonal\" % (diff, seasonal_diff))\n",
    "    plt.show() \n",
    "    \n",
    "X_linear_trend = generate_timeseries(deg='linear')\n",
    "X_seasonal_trend = generate_timeseries(deg='sinusoidal')\n",
    "\n",
    "X_both = X_linear_trend + X_seasonal_trend\n",
    "\n",
    "trend_removal_via_seasonal_nonseasonal_differencing(X_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "1. Did it work? Does the data now look stationary, there is no underlying direction to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: ARIMA using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/TSLA.csv')\n",
    "time = np.array(data['Date'])\n",
    "price = np.array(data['Close'])\n",
    "\n",
    "plt.title('Tesla Stock Price')\n",
    "plt.plot(time, price)\n",
    "plt.xticks(time[::50])\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above depicts the stock price of TSLA over time. The x-axis being time and the y-axis being f(t), the price at that point in time.\n",
    "\n",
    "Clearly a basic linear model wouldn't be able to model such a function of time due to its non-linearity. However, we have previously learned ways to fit non-linear functions with linear regression using feature lifting. Maybe lifting some polynomial features might help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Linear Regression\n",
    "\n",
    "Linear features, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = np.arange(0, time.shape[0], 1).astype(np.int).reshape((time.shape[0], 1))\n",
    "\n",
    "#TODO: run a least squares linear regression in the format Ax=b, A = time_steps, B = price\n",
    "#place the result in w\n",
    "#Hint: try np.linalg.lstsq\n",
    "\n",
    "#YOUR CODE HERE\n",
    "w = None\n",
    "#END YOUR CODE\n",
    "\n",
    "plt.plot(time_steps, w * time_steps, label='linear fit')\n",
    "plt.plot(time_steps, price, label = 'original data')\n",
    "plt.legend()\n",
    "plt.title('Tesla Stock Price with linear fit')\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial features, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_lift(X, d=2):\n",
    "    \"\"\"\n",
    "    This function computes second order variables\n",
    "    for polynomial regression.\n",
    "    Input:\n",
    "    X: Independent variables.\n",
    "    Output:\n",
    "    A data matrix composed of both first and second order terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    X_data = []\n",
    "    \n",
    "    for deg in range(1, d+1):\n",
    "        X_data.append(np.power(X, deg))\n",
    "        \n",
    "    return np.hstack(X_data)\n",
    "\n",
    "second_order_time_steps = feature_lift(time_steps, d=2)\n",
    "w_2, err_2, _, _ = np.linalg.lstsq(second_order_time_steps[:210], price[:210])\n",
    "\n",
    "third_order_time_steps = feature_lift(time_steps, d=3)\n",
    "w_3, err_3, _, _ = np.linalg.lstsq(third_order_time_steps[:220], price[:220])\n",
    "\n",
    "fifty_order_time_steps = feature_lift(time_steps, d=50)\n",
    "w_50, err_50, _, _ = np.linalg.lstsq(fifty_order_time_steps[:220], price[:220])\n",
    "\n",
    "plt.plot(time_steps, second_order_time_steps @ w_2.T, label='deg 2 fit')\n",
    "plt.plot(time_steps, third_order_time_steps @ w_3.T, label='deg 3 fit')\n",
    "plt.plot(time_steps, fifty_order_time_steps @ w_50.T, label='deg 50 fit')\n",
    "plt.plot(time_steps, price, label = 'original data')\n",
    "\n",
    "plt.xlim(right=250)\n",
    "plt.xlabel('Days since November 19, 2019')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.title('Tesla stock price with linear regression on feature lift')\n",
    "plt.show()\n",
    "\n",
    "print('Training error with degree 2 features: %s' % str(err_2))\n",
    "print('Training error with degree 3 features: %s' % str(err_3))\n",
    "print('Training error with degree 50 features: %s' % str(err_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, any attempt to fit some sort of function using linear regression and feature lifting will be met with failure. Why? This is due to the nature of time series data. The problem that linear regressions try to solve is fitting some function to the data in its entirety. However, time series data, such as stock prices don't operate under the same assumptions. Does the stock price 1 year ago directly affect the stock price tomorrow? The answer is, not really. This understanding that only the previous n' << n observations are truly important to our model spawns this set of models that utilize the moving average, the first MA model, then the ARMA, and ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation of the ARIMA Model\n",
    "\n",
    "The mathematical model behind ARIMA is:\n",
    "\n",
    "\\begin{align*}\n",
    "ARIMA(p, d, q) = (1 - \\phi_{1}L - \\phi_{2}L^{2} - \\phi_{3}L^{3}... - \\phi_{p}L^{p}) (1 - L)^{d} y_{t} = c + (1 + \\theta_{1} L + \\theta_{2} L^{2}... + \\theta_{q} L^{q}) \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "The L is the lag operator, $L^d y_{t} = y_{t - d}$.\n",
    "\n",
    "The first group of terms, containing $\\phi$'s, are the auto-regressive terms. Auto-regressive refers to the contribution of previous observations on the current observation. The parameter $p$ is the number of previous observations looked at. The $\\phi$'s are their individual weighting. Intuitively, an observation 1 time step ago usually holds more weight than an observation 10 time steps ago.\n",
    "\n",
    "The second term, $(1 - L)^{d}$, is the integrated term. This is the term that implements the idea of differencing, which is sometimes necessary to find a deeper relationship in the data. The time series function may not be able to model the observation themselves, but may be able to model the difference between successive observations.\n",
    "\n",
    "The third and final term, containing $\\theta$'s, represent the moving average portion of the model. This is the contribution of the previous $q$ terms, specifically their residuals to the residual at time t. The residuals, $\\epsilon_{t}$, are standard normal distribution deviations of each observation from the predicted observation predicted by the model. These residuals are what we try to minimize when fitting parameters to the training data.\n",
    "\n",
    "The constant c quantifies the drift of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down the ARIMA Model\n",
    "\n",
    "### Auto-Regressive (AR)\n",
    "\n",
    "One of the fundamental building blocks of the ARIMA model is the AR portion of the model. The equation for a simple AR model is:\n",
    "\n",
    "\\begin{align*}\n",
    "AR(p) = y_{t} = c + (\\phi_{1}L + \\phi_{2}L^{2} + \\phi_{3}L^{3}... + \\phi_{p}L^{p})y_{t} + \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "This equation can already begin to model functions under the assumption that current observations are simply derived from a linear combination of the p previous observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this part of the notebook, we'll be working with weather data instead of stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "weather_data = pd.read_csv('data/weather_data.csv', parse_dates=['datetime_utc'], index_col='datetime_utc')\n",
    "weather_data = weather_data.rename(index=str, columns={' _tempm': 'temperature'})\n",
    "\n",
    "#interpolate null values\n",
    "weather_data.ffill(inplace=True)\n",
    "weather_data.index = pd.to_datetime(weather_data.index)\n",
    "\n",
    "#remove outliers\n",
    "weather_data = weather_data[weather_data.temperature < 50]\n",
    "weather_temp = weather_data['temperature']\n",
    "\n",
    "weather_temp.plot()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('temp in deg C')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Historical weather data')\n",
    "plt.show()\n",
    "\n",
    "print(weather_temp.head())\n",
    "print('Null values: %s' % str(weather_temp.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do with our data is clean it, interpolating null values and making sure the samples are evenly spaced. Next, there are some things we can check about the data itself to ensure it is suitable for an ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = weather_temp['2000':'2014'].resample('M').mean().fillna(method='pad')\n",
    "testing_data = weather_temp['2015':'2017'].resample('M').mean().fillna(method='pad')\n",
    "training_data.plot()\n",
    "testing_data.plot()\n",
    "plt.title('Training and Testing Data Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will plot the autocorrelation of both the original data and the differences between consecutive data points. There is also a provided function ``plot_rolling_mean_std``. Try to find a period length that creates a constant rolling mean. What does this say about the periodicity of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rolling mean and rolling standard deviation\n",
    "def plot_rolling_mean_std(ts, period):\n",
    "    rolling_mean = ts.rolling(period).mean()\n",
    "    rolling_std = ts.rolling(period).std()\n",
    "\n",
    "    plt.plot(ts, label='Actual Mean')\n",
    "    plt.plot(rolling_mean, label='Rolling Mean')\n",
    "    plt.plot(rolling_std, label = 'Rolling Std')\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Mean Temperature\")\n",
    "    plt.title('Rolling Mean & Rolling Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#TODO: change this period parameter until the resulting rolling mean is 0 (Hint: What do you think the period of this data is?)\n",
    "plot_rolling_mean_std(training_data, period = 1)\n",
    "    \n",
    "plt.plot(training_data)\n",
    "plot_acf(training_data)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_data.diff())\n",
    "plot_acf(training_data.diff().values)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the rolling mean and rolling standard deviation plot that the data has a constant mean and standard deviation on a 12 month period. This periodicity implies that our data is seasonal, meaning a standard ARIMA model will have trouble fitting it, but instead we will need to use the seasonal aspect of SARIMA to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will try to see what kind of model we retrieve from an AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(res, model='arima'):\n",
    "    print(res.summary())\n",
    "    \n",
    "    plt.plot(res.resid)\n",
    "    plt.title('Training Residuals')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "\n",
    "    print('Mean squared training error: %s' % str(np.square(res.resid).mean()))\n",
    "\n",
    "    plt.plot(res.predict(), label='Prediction')\n",
    "    plt.plot(training_data.values, label='Real values')\n",
    "    plt.legend()\n",
    "    plt.title('Model fit on training data')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "    \n",
    "    if model == 'arima':\n",
    "        fc, se, conf = res.forecast(testing_data.shape[0], alpha=0.05)\n",
    "    elif model == 'sarimax':\n",
    "        fc = res.forecast(testing_data.shape[0], alpha=0.05)\n",
    "        \n",
    "    fc_series = pd.Series(fc, index=testing_data.index)\n",
    "    plt.plot(fc_series, label='Prediction')\n",
    "    plt.plot(testing_data, label='Real values')\n",
    "    plt.legend()\n",
    "    plt.title('Model prediction on test data')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Temperature in C')\n",
    "    plt.show()\n",
    "    \n",
    "    #TODO: Calculate the mean squared testing error\n",
    "    #fc = prediction, testing_data = true observations\n",
    "    squared_testing_error = 0\n",
    "    print('Mean squared testing error: %s' % str(squared_testing_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use ARIMA(data, order).fit() to generate an AR model fit using order=(p, 0, 0)\n",
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html\n",
    "#Hint: try p > 1\n",
    "res = ...\n",
    "process_result(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Why does the AR model not fit this seasonal data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average (MA)\n",
    "\n",
    "Another fundamental building block of the ARIMA model is the MA portion of the model. The equation for a simple MA model is:\n",
    "\n",
    "\\begin{align*}\n",
    "MA(q) = y_{t} = c + \\epsilon_{t} + \\theta_{1}\\epsilon_{t-1} + \\theta_{2}\\epsilon_{t-2} + ... + \\theta_{q}\\epsilon_{t-q}\n",
    "\\end{align*}\n",
    "\n",
    "This equation models the assumption that the residual, the deviation from the predicted point, of the current observation is a linear combination of the previous q residuals.\n",
    "\n",
    "Add the moving average section of the ARIMA model, and model the weather data using an ARMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use ARIMA(data, order).fit() to generate an ARMA model fit using order=(p, 0, q)\n",
    "#https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html\n",
    "#Hint: try p, q > 1\n",
    "res = ...\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Does the ARMA model fit the seasonal data better than the AR model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality in ARIMA: SARIMA\n",
    "\n",
    "As we noted in the weather data, we see a seasonal pattern, or period with fixed period length. The traditional ARIMA model is not fully suited to model this behavior, but this is where SARIMA comes into play. SARIMA, season auto-regressive integrated moving average, has a slightly different formulation from ARIMA:\n",
    "\n",
    "\\begin{align*}\n",
    "SARIMA(p, d, q) (P, D, Q)_{m} = (1 - \\phi_{1}L - \\phi_{2}L^{2} - \\phi_{3}L^{3}... - \\phi_{p}L^{p}) (1 - \\Phi_{1}L^{m} - \\Phi_{2}L^{2m} - \\Phi_{3}L^{3m}... - \\Phi_{P}L^{Pm}) (1 - L)^{d} (1 - L)^{Dm} y_{t} = c + (1 + \\theta_{1} L + \\theta_{2} L^{2}... + \\theta_{q} L^{q}) (1 + \\Theta_{1} L^{m} + \\Theta_{2} L^{2m}... + \\Theta_{Q} L^{Qm}) \\epsilon_{t}\n",
    "\\end{align*}\n",
    "\n",
    "The seasonal terms are terms that operate much like the original ARIMA terms, but the lag operators are all raised to the power of $m$, in the case of the annual cycle of weather data, $m=12$, meaning the model takes into account observations from 12, 24, 36.... months ago.\n",
    "\n",
    "Implement a SARIMA model on the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use SARIMAX(data, order, seasonal_order).fit()\n",
    "#https://www.statsmodels.org/stable/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.fit.html\n",
    "#try p, q, P, Q > 1\n",
    "res = ...\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write down your observations.\n",
    "\n",
    "How does the model perform on the training data?\n",
    "\n",
    "How does the model perform on the forecasting of future data?\n",
    "\n",
    "Does the SARIMAX model fit the seasonal data better than the ARIMA model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluating your ARIMA model\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "You may have noticed that sometimes the performance is better than other times when using different values for p, d, q, P, D, and Q. These values are called hyperparameters, hand selected values by the engineer creating the model that greatly influence the effectiveness of the model. There are several ways to set these hyperparameters, as discussed earlier in this course. You can employ techniques such as k-fold cross validation and grid searches, as well as use domain knowledge to make educated guesses on what we want the model to learn and what the underlying model in reality is. For example, someone with strong knowledge of the weather can make a better educated guess about how many days one should look back in the past to predict today's weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO run a SARIMAX model with your guess\n",
    "res = ...\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try using a grid search to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_mse(res):\n",
    "    #TODO calculate test mse\n",
    "    #use res.forecast(testing_data.shape[0]) to get predictions\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    fc = ...\n",
    "    squared_testing_error = ...\n",
    "    \n",
    "    #END YOUR CODE\n",
    "    return squared_testing_error\n",
    "\n",
    "p_range = list(range(3))\n",
    "q_range = list(range(3))\n",
    "P_range = list(range(3))\n",
    "Q_range = list(range(3))\n",
    "\n",
    "best_params = (-1, -1, -1, -1)\n",
    "best_test_mse = 10000000\n",
    "\n",
    "#TODO: use a grid search to find the best parameters\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        for P in P_range:\n",
    "            for Q in Q_range:\n",
    "                #TODO: run SARIMAX on training_data.values, order = (p, 0, q), seasonal_order = (P, 0, Q, 12)\n",
    "                #call your function get_test_mse(sarimax result)\n",
    "                #save if new best test mse\n",
    "                \n",
    "                #YOUR CODE HERE\n",
    "                \n",
    "                #END YOUR CODE\n",
    "                \n",
    "                \n",
    "print('Best parameters: ', best_params, ' Best Test MSE: ', best_test_mse)\n",
    "res = SARIMAX(training_data.values, order=(best_params[0], 0, best_params[1]), seasonal_order=(best_params[2], 0, best_params[3], 12)).fit()\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That probably took a while, was it worth it?\n",
    "\n",
    "### Observations\n",
    "\n",
    "How did the grid search perform compared to your guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Model: AIC\n",
    "\n",
    "Not only do we need to tune the hyperparameters to increase the performance of our model, but we also need a way to evaluate our model. We need a measure that maximizes the accuracy of the model while minimizing the number of parameters of our model, to avoid overfitting our training data. This is where AIC comes in. AIC is short for Akaike information criterion. It is formulated as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "AIC = -2log(L) + 2(p + q + k)\n",
    "\\end{align*}\n",
    "\n",
    "k = 1 if the model includes an intercept, else k = 0.  \n",
    "L = likelihood of our data (this depends on the residuals and dist. of white noise)\n",
    "\n",
    "We want our model to minimize this quantity.  \n",
    "Intuitively, we want to maximize the likelihiood of our data while our model is penalized by the complexity of the data.\n",
    "\n",
    "Perform the grid search again on the hyperparameters while instead minimizing AIC instead of test_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_range = list(range(3))\n",
    "q_range = list(range(3))\n",
    "P_range = list(range(2))\n",
    "Q_range = list(range(2))\n",
    "\n",
    "best_params = (-1, -1, -1, -1)\n",
    "best_aic = 10000000\n",
    "\n",
    "#TODO: use a grid search to find the best parameters\n",
    "for p in p_range:\n",
    "    for q in q_range:\n",
    "        for P in P_range:\n",
    "            for Q in Q_range:\n",
    "                #TODO: run SARIMAX on training_data.values, order = (p, 0, q), seasonal_order = (P, 0, Q, 12)\n",
    "                #save if new best aic\n",
    "                \n",
    "                #YOUR CODE HERE\n",
    "                \n",
    "                #END YOUR CODE\n",
    "                \n",
    "                \n",
    "print('Best parameters: ', best_params, ' Best AIC: ', best_aic)\n",
    "res = SARIMAX(training_data.values, order=(best_params[0], 0, best_params[1]), seasonal_order=(best_params[2], 0, best_params[3], 12)).fit()\n",
    "process_result(res, 'sarimax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. Did the best test MSE model differ from the best AIC model?\n",
    "2. Which one had smaller parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implementing an AR model solver\n",
    "\n",
    "As we noted in the notes, implementing an AR solver is much easier than an MA solver. This is because an AR solver models the each observation as a linear combination of $p$ previous observations, while an MA solver involves residuals which comes as a result of the model itself, requiring more complicated solving methods. Interestingly, we found in the notes in the section on invertibility that oftentimes MA models can be recast as an AR model. Refer to section 6.1 of the notes for further reading.\n",
    "\n",
    "In the following part, we will implement an AR solver ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the dataset inside data/weather_data.csv, like Part 3 of the notebook. We will build an AR model solver and test it on this dataset, but using a different observation than temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "weather_data = pd.read_csv('data/weather_data.csv', parse_dates=['datetime_utc'], index_col='datetime_utc')\n",
    "weather_data = weather_data.rename(index=str, columns={' _tempm': 'temperature', ' _hum': 'humidity', ' _dewptm': 'dew_point_temp', ' _pressurem': 'pressure'})\n",
    "weather_data.index = pd.to_datetime(weather_data.index)\n",
    "\n",
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a column from weather_data and create a new dataframe from weather_data with only that column.  \n",
    "Note: the keys in the dataframe may have some whitespace so make sure you are spelling everything correctly to index the column.  \n",
    "\n",
    "Then, we need to interpolate null values and remove outliers (clean the data).  \n",
    "Refer to the code blocks in Part 3 or check out Pandas documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Choose a column from weather_data (solution uses humidity)\n",
    "\n",
    "#YOUR CODE HERE\n",
    "weather_data_one_col = ...\n",
    "\n",
    "#TODO: interpolate null values\n",
    "#reference the appropriate code section from Part 3\n",
    "\n",
    "\n",
    "#TODO: remove outliers\n",
    "#reference the appropriate code section from Part 3\n",
    "weather_data_one_col = ...\n",
    "\n",
    "#END YOUR CODE\n",
    "\n",
    "weather_data_one_col.plot()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('Humidity')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Historical weather data')\n",
    "plt.show()\n",
    "\n",
    "print(weather_data_one_col.head())\n",
    "print('Null values: %s' % str(weather_data_one_col.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to resample the data to regular intervals as well as split the data into training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = weather_data_one_col['2000':'2014'].resample('M').mean().fillna(method='pad')\n",
    "testing_data = weather_data_one_col['2015':'2017'].resample('M').mean().fillna(method='pad')\n",
    "training_data.plot()\n",
    "testing_data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill out the AR_Model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AR_Model():\n",
    "    def __init__(self, q):\n",
    "        #TODO: initialize variables, for the AR Model (q, phi's)\n",
    "        #Think: How many phi's do we need\n",
    "        \n",
    "        #YOUR CODE HERE\n",
    "        self.phi = ...\n",
    "        #END YOUR CODE\n",
    "        \n",
    "    def fit(self, data):\n",
    "        #TODO: fit parameters to the data\n",
    "        #steps:\n",
    "        #create a data matrix (A), each row should have q previous observations\n",
    "        #create a label vector (b), each element should be one observation\n",
    "        #solve least squares Ax = b, x = phi's\n",
    "        \n",
    "        #return a vector of observations predicted from the fitted model, the same shape as data\n",
    "        #multiply data matrix against the predicted model phi's and return the output vector\n",
    "        \n",
    "        #need to keep this for predicting\n",
    "        self.data = data\n",
    "        \n",
    "        #YOUR CODE HERE\n",
    "        data_mat = []\n",
    "        label_vector = []\n",
    "        for i in range(data.shape[0] - self.phi.shape[0]):\n",
    "            data_mat.append(...)\n",
    "            label_vector.append(...)\n",
    "            \n",
    "        data_mat = np.array(data_mat)\n",
    "        label_vector = np.array(label_vector)\n",
    "        \n",
    "        w = ...\n",
    "        self.phi = ...\n",
    "        \n",
    "        #don't predict anything for first q observations as there are not q previous observations for the AR Model\n",
    "        predicted = np.zeros(self.phi.shape[0])\n",
    "        return ...\n",
    "        #END YOUR CODE\n",
    "    \n",
    "        \n",
    "    def predict(self, n):\n",
    "        #TODO: predict the next n observations given the fitted model\n",
    "        \n",
    "        #YOUR CODE HERE\n",
    "        assert(len(self.data) > 0)\n",
    "        data = list(np.copy(self.data))\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(n):\n",
    "            next_prediction = ...\n",
    "            data.append(next_prediction)\n",
    "            predictions.append(next_prediction)\n",
    "            \n",
    "        return np.array(predictions)\n",
    "        \n",
    "        #END YOUR CODE\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit and Run the following code to test your model and plot against both training and testing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 10\n",
    "arm = AR_Model(q)\n",
    "\n",
    "model_fit_training = arm.fit(training_data)\n",
    "\n",
    "plt.plot(training_data.values, label='training data')\n",
    "plt.plot(model_fit_training, label='model fit')\n",
    "plt.title('Student implemented AR Model with parameter %d' % q)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Observations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "predicted = arm.predict(testing_data.shape[0])\n",
    "\n",
    "plt.plot(testing_data.values, label='testing data')\n",
    "plt.plot(predicted, label='model predicted')\n",
    "plt.title('Student implemented AR Model predictions with parameter %d' % q)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Observations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the statsmodels implementation of an AR model on the same data. Yours should look similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ARIMA(training_data.values, order=(q, 0, 0)).fit()\n",
    "process_result(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should run much faster than the statsmodels package. This is because we are implementing a very simple solver for the AR model parameters. If you would like to find out more about more complex and precise solvers, such as Maximum Likelihood Estimators, check out statsmodels opensource code hosted on github: https://github.com/statsmodels/statsmodels. Although we only wrote an AR solver in this part of the code, we derived the invertibility of MA models to AR models in the notes, so this solver could be applicable to any ARMA model. Supporting ARIMA models would require simply adding a differencing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cool ARIMA models\n",
    "\n",
    "There are some cool patterns that can be modeled using ARIMA. \n",
    "Do not use statsmodels.\n",
    "\n",
    "1. Random Walk: ARIMA(0, 1, 0)\n",
    "2. Random Walk with drift: ARIMA(0, 1, 0) w/ a constant\n",
    "\n",
    "\n",
    "First: Derive the explicit formulation of each cool pattern as $X_t = $:\n",
    "\n",
    "Random Walk:\n",
    "\n",
    "Random Walk with drift:\n",
    "\n",
    "Implement the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(num_steps=100):\n",
    "    state = 0\n",
    "    ts = []\n",
    "    \n",
    "    #TODO: fill ts with num_steps predictions from random_walk model\n",
    "    #use np.random.normal(0, 1) for Zt\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    for i in range(num_steps):\n",
    "        ts.append(0)\n",
    "        \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    return ts\n",
    "\n",
    "def random_walk_with_drift(num_steps=100, drift=0.4):\n",
    "    state = 0\n",
    "    ts = []\n",
    "    \n",
    "    #TODO: fill ts with num_steps predictions from random_walk model\n",
    "    #use np.random.normal(0, 1) for Zt\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    for i in range(num_steps):\n",
    "        ts.append(0)\n",
    "        \n",
    "    #END YOUR CODE\n",
    "        \n",
    "    return ts\n",
    "    \n",
    "\n",
    "random_walk_ts = random_walk()\n",
    "random_walk_drift_ts = random_walk_with_drift()\n",
    "dampled_holts_ts = dampled_holts()\n",
    "\n",
    "plt.plot(random_walk_ts)\n",
    "plt.title(\"Random Walk\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"observation\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(random_walk_drift_ts)\n",
    "plt.title(\"Random Walk with drift\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"observation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "1. Run the above cell a few times. What does the Random Walk look like? What does the Random Walk with drift look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "This notebook provides an introduction to time series modeling, first exploring the concepts behind smoothing and stationarity, then using the open source statsmodels library to generate and fit ARIMA models, evaluating our ARIMA models, and finally exploring different patterns that can be modeled using ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
